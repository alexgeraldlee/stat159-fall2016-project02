<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Alexander Lee and Youngshin Kim" />
  <title>Predictive Modeling Processes on Credit Data</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="header">
<h1 class="title">Predictive Modeling Processes on Credit Data</h1>
<h2 class="author">Alexander Lee and Youngshin Kim</h2>
<h3 class="date">November 4, 2016</h3>
</div>
<h1 id="abstract">Abstract</h1>
<p>This project functions as an introduction to ridge regression, lasso regression, principal components regression, and partial least squares regression, particularly in R. We use R to run each of these regressions and compare the coefficients determined by these regressions to those generated by the original least squares regression.</p>
<h1 id="introduction">Introduction</h1>
<p>This project aims to predict data using different methods. Methods used are original least squares regression, ridge, lasso, principal component regression, and partial least sqaures regression. We want to be able to predict variable 'Balance' from ten predictors. Before we begin, we will first make some necessary changes to our dataset using data manipulation in R. #Data</p>
<p>The data we use in this project is the credit data and it includes variable Balance, the variable we are trying to predict, and ten predictors that we use to predict Balance. These predictors are Income, Limit, Rating, Cards, Age, Education, GenderFemale, StudentYes, MarriedYes, EthnicityAsian, EthnicityCaucasian. The last two predictors were originally one predictor, but it was expanded as a result of dummifying the values. #Methods</p>
<p>We use five regression methods in this project: OLS, ridge, lasso, pcr, plsr and check the method that gives us the lowest MSE. OLS is the most simple regression method that tries to fit a line that minimizes the residual sum of squares. The rest are all regularization methods. Ridge and lasso regressions are shrinkage methods that put a penalty to our linear model and we try to find the best model with the smallest lambda. Lasso is different from ridge in that lasso does variable selection. It cuts out predictors that are not useful to predicting Balance. PCR and PLSR regressions perform dimension reduction, and are useful when the predictors are correlated to each other.</p>
<h1 id="analysis">Analysis</h1>
<p>For OLS, we simply fit the lm function in R. For the rest of the methods, we first use 10 fold cross-validation to fit method on train set and find the best model based on minimum lambda for ridge/lasso and minimum prediction error sum of squares for pcr and plsr. Next, we apply the best model to test set and calculate MSE. Finally, we choose the regression method that gives us the lowest MSE.</p>
<h1 id="results">Results</h1>
<p><code>{r, echo = FALSE} load('../../data/best-ols.RData') load('../../data/best-ridge.RData') load('../../data/best-lasso.RData') load('../../data/best-pcr.RData') load('../../data/best-plsr.RData') library(xtable) options(xtable.comment = FALSE)</code></p>
<p>```{r, echo = FALSE, comment = NA, results = 'asis'} reg_coef = matrix(c(ols_coefficients[-1], best_ridge, best_lasso, best_pcr, best_plsr), nrow = 11)</p>
<p>rownames(reg_coef) = c('Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'GenderFemale', 'StudentYes', 'MarriedYes', 'EthnicityAsian', 'EthnicityCaucasian')</p>
<p>colnames(reg_coef) = c('OLS', 'Ridge', 'Lasso', 'PCR', 'PLSR')</p>
<p>reg_coef = as.data.frame(reg_coef) print(xtable(reg_coef, caption = 'Regression Coefficients for All Methods')) ```</p>
<p>The table above displays the regression coefficients of the best model for each method. There are some things that stand out. First, the OLS coefficients seem very different from the others. This was expected, because OLS is the weakest method in this project. Second, ridge/lasso and pcr/plsr seem to share similar regression coefficients values. This is also expected, because each pair is similar to each other.</p>
<p>```{r, echo = FALSE, comment = NA, results = 'asis'} mse = matrix(c(ridge_mse, lasso_mse, pcr_mse, plsr_mse), nrow = 1)</p>
<p>rownames(mse) = 'MSE'</p>
<p>colnames(mse) = c('Ridge', 'Lasso', 'PCR', 'PLSR')</p>
<p>mse = as.data.frame(mse) print(xtable(mse, caption = 'MSEs for All Methods')) ```</p>
<p>The table above shows the mean squared errors for each method. These values were calculated by applying the best model to the test set and calculating the mean of squared difference between estimated value and actual value (Balance). It is clear that PCR and PLSR have low MSEs compared to the other methods.</p>
<p>```{r, echo = FALSE, comment = NA, results = 'asis'} library(ggplot2)</p>
<p>official = matrix(c(ols_coefficients, ridge_coefficients, lasso_coefficients,pcr_coefficients, plsr_coefficients), nrow = 12)</p>
<p>colnames(official) = c('OLS', 'Ridge', 'Lasso', 'PCR', 'PLSR') rownames(official) = c('Intercept', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'GenderFemale', 'StudentYes', 'MarriedYes', 'EthnicityAsian', 'EthnicityCaucasian')</p>
<p>label = c('Int', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Edu', 'Gender', 'Student', 'Married', 'Asian', 'Caucasian')</p>
<p>plot(official[,1], main = 'OLS official coefficients', xlab = 'Predictors', ylab = 'Balance', xaxt = 'n', col = 'red') axis(1, at = 1:12, labels = label)</p>
<p>plot(official[,2], main = 'Ridge official coefficients', xlab = 'Predictors', ylab = 'Balance', xaxt = 'n', col = 'red') axis(1, at = 1:12, labels = label)</p>
<p>plot(official[,3], main = 'Lasso official coefficients', xlab = 'Predictors', ylab = 'Balance', xaxt = 'n', col = 'red') axis(1, at = 1:12, labels = label)</p>
<p>plot(official[,4], main = 'PCR official coefficients', xlab = 'Predictors', ylab = 'Balance', xaxt = 'n', col = 'red') axis(1, at = 1:12, labels = label)</p>
<p>plot(official[,5], main = 'PLSR official coefficients', xlab = 'Predictors', ylab = 'Balance', xaxt = 'n', col = 'red') axis(1, at = 1:12, labels = label) ```</p>
<p>These are the 'official' regression coefficients that were calculated by applying the best model to the full data set. #Conclusion</p>
<p>From the tables and figures above, we can conclude that PCR/PLSR give us the best estimate of Balance. We can make this conclusion, because PCR/PLSR gave us the lowest MSE (0.05), which means the predicted values and the actual values weren't that much different from each other. Also, from the figures above, we can assume that there were only a couple predictors that were actually important in estimating Balance. The most noticeable predictors are Limit and Student. It's clear in the figures that the coefficients are high for these predictors.</p>
</body>
</html>
